{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45addd7-4959-43b4-8afa-f08a56f8ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib\n",
    "\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from utils import *\n",
    "from lm_saliency import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0551590-fe14-4821-b547-562051a983ca",
   "metadata": {},
   "source": [
    "## calculate saliency scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ba6b0-e425-4301-8843-e482147348be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model, task, and prompt\n",
    "pretrains = ['google/flan-t5-large']\n",
    "datasets = ['cola']\n",
    "prompt_types = ['standard_b']\n",
    "seeds = [2266]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for prompt_type in prompt_types:\n",
    "        for n_o_s in number_of_shots[dataset]:\n",
    "            for seed in seeds:\n",
    "                for pretrained in pretrains:\n",
    "                    \n",
    "                    tokenizer = AutoTokenizer.from_pretrained(pretrained, padding_side='left')\n",
    "                    tokenizer.add_special_tokens({'pad_token': pad_tokens[pretrained]})\n",
    "                    if pretrained in ['google/flan-t5-large']:\n",
    "                        model = T5ForConditionalGeneration.from_pretrained(pretrained).to(device)\n",
    "                    else:\n",
    "                        model = AutoModelForCausalLM.from_pretrained(pretrained).to(device)\n",
    "                    \n",
    "                    config = {\n",
    "                        'experiment_id': time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n",
    "                        'dataset': dataset,\n",
    "                        'number_of_data': None,\n",
    "                        'model': pretrained,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'number_of_shots': n_o_s,\n",
    "                        'temperature': 0.8,\n",
    "                        'max_tokens': max_tokens[prompt_type],\n",
    "                        'batch_size': 16,\n",
    "                        'eos_token_id': eos_ids[pretrained],\n",
    "                        'seed': seed,\n",
    "                        'device': torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "                    }\n",
    "\n",
    "                    torch.cuda.manual_seed_all(config['seed'])\n",
    "\n",
    "                    # read in the prompt\n",
    "                    with open(f'prompts/{dataset}/{prompt_type}-{n_o_s}.txt') as file:\n",
    "                        prefix = file.read()\n",
    "                    \n",
    "                    # read in the data\n",
    "                    with open(f'datasets/{dataset}.json') as file:\n",
    "                        data = json.loads(file.read())\n",
    "                    config['number_of_data'] = len(data)\n",
    "\n",
    "                    # calculate the saliency scores\n",
    "                    l1_norms, input_x_gradients = defaultdict(list), defaultdict(list)\n",
    "                    with tqdm(total=config['number_of_data']) as t:\n",
    "                        for i, item in data.items():\n",
    "                            inputs = []\n",
    "                            inputs.append(concatenate(dataset, prompt_type, prefix, item['original'], item))\n",
    "                            for synthetic in list(item['synthetic'].values())[:4]:\n",
    "                                inputs.append(concatenate(dataset, prompt_type, prefix, synthetic, item))\n",
    "                            tokenized = tokenizer(inputs)\n",
    "                            all_input_tokens, all_attention_ids = tokenized['input_ids'], tokenized['attention_mask']\n",
    "                            for _ in range(len(inputs)):\n",
    "                                input_tokens = all_input_tokens[_]\n",
    "                                attention_ids = all_attention_ids[_]\n",
    "                                base_saliency_matrix, base_embd_matrix = saliency(model, input_tokens, attention_ids, pretrained)\n",
    "                                l1_norms[i].append(l1_grad_norm(base_saliency_matrix, normalize=True))\n",
    "                                input_x_gradients[i].append(input_x_gradient(base_saliency_matrix, base_embd_matrix, normalize=True))\n",
    "                            t.update(1)\n",
    "                    \n",
    "                    for t in [l1_norms, input_x_gradients]:\n",
    "                        for _, v in t.items():\n",
    "                            for i in range(len(v)):\n",
    "                                if pretrained not in ['google/flan-t5-large']:\n",
    "                                    v[i] = list(v[i])\n",
    "                                else:\n",
    "                                    v[i] = list(v[i][1])\n",
    "                                for j in range(len(v[i])):\n",
    "                                    v[i][j] = float(v[i][j])\n",
    "                    \n",
    "                    # save the results\n",
    "                    with open('config_reference_saliency.txt', 'a') as file:\n",
    "                        file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "                        file.close()\n",
    "                    with open('results_saliency/l1_norms-{}.json'.format(config['experiment_id']), 'w') as file:\n",
    "                        json.dump(l1_norms, file, indent=4, ensure_ascii=False)\n",
    "                    with open('results_saliency/input_x_gradients-{}.json'.format(config['experiment_id']), 'w') as file:\n",
    "                        json.dump(input_x_gradients, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "                    del model\n",
    "                    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c34ea-ec70-40af-8e77-2e461e55117c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3ede6-de11-4fa8-ab5b-0eaf83d18dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model, task, prompt, and seed\n",
    "model = 'togethercomputer/GPT-JT-6B-v1'\n",
    "dataset = 'cola'\n",
    "prompt_type = 'standard_b'\n",
    "seed = 2266\n",
    "\n",
    "# read in the data\n",
    "with open(f'datasets/{dataset}.json') as file:\n",
    "    data = json.loads(file.read())\n",
    "\n",
    "# specify the number of instances to show\n",
    "instances = [str(_) for _ in range(5)]\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side='left')\n",
    "tokenizer.add_special_tokens({'pad_token': pad_tokens[model]})\n",
    "\n",
    "for experiment_id in config_df_saliency.experiment_id.tolist():\n",
    "    config = {k: v[0] for k, v in config_df_saliency[config_df_saliency.experiment_id == experiment_id].to_dict(orient='list').items()}\n",
    "    if config['dataset'] == dataset and config['model'] == model and int(config['seed']) == seed and config['prompt_type'] == prompt_type:\n",
    "        \n",
    "        # read in the saliency scores\n",
    "        with open(f'results_saliency/l1_norms-{experiment_id}.json') as file:\n",
    "            target = json.loads(file.read())\n",
    "        \n",
    "        # read in the prompt\n",
    "        n_o_s = number_of_shots[dataset][0]\n",
    "        with open(f'prompts/{dataset}/{prompt_type}-{n_o_s}.txt') as file:\n",
    "            prefix = file.read()\n",
    "            \n",
    "        random.seed(seed)\n",
    "        for i in instances:\n",
    "            item = data[i]\n",
    "            inputs = [concatenate(dataset, prompt_type, prefix, item['original'], item)]\n",
    "            for synthetic in list(item['synthetic'].values())[:4]:\n",
    "                inputs.append(concatenate(dataset, prompt_type, prefix, synthetic, item))\n",
    "            tokenized = tokenizer(inputs)\n",
    "            all_input_tokens, all_attention_ids = tokenized['input_ids'], tokenized['attention_mask']\n",
    "\n",
    "            for j in range(0, 1):\n",
    "                if model not in ['google/flan-t5-large']:\n",
    "                    sep = 198\n",
    "                    if prompt_type in ['standard_a']:\n",
    "                        start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-2] + 1\n",
    "                    if prompt_type in ['standard_b', 'CoT', 'context faithful prompting']:\n",
    "                        start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-3] + 1\n",
    "                    if prompt_type in ['zero_sensitivity_b']:\n",
    "                        start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-3] + 2\n",
    "                    if prompt_type in ['gkp']:\n",
    "                        start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-4] + 1\n",
    "                    target[i][j] = [_*10 for _ in target[i][j]]\n",
    "                else:\n",
    "                    sep = 10\n",
    "                    if prompt_type in ['zero_sensitivity_b']:\n",
    "                        if dataset in ['cola']:\n",
    "                            start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-3] + 2\n",
    "                        if dataset in ['sst2']:\n",
    "                            start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-3] + 3\n",
    "                        if dataset in ['csqa', 'mnli', 'rte']:\n",
    "                            start_id = [_ for _, x in enumerate(all_input_tokens[j]) if x == sep][-4] + 2\n",
    "                \n",
    "                # normalize the scores\n",
    "                norm = mpl.colors.Normalize(vmin=-0.98, vmax=1)\n",
    "                \n",
    "                # display a heatmap\n",
    "                tokens = [tokenizer.decode(_) for _ in all_input_tokens[j][start_id:]]\n",
    "                attention = np.array(target[i][j][len(target[i][j])-(len(all_input_tokens[j])-start_id):])\n",
    "                attention = norm(attention)\n",
    "                s = colorize(tokens, attention)\n",
    "                display(HTML(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b448e-4496-4cbd-9e25-344238734a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
