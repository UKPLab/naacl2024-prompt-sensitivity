{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f467238-789c-41fd-9c1c-d15b80d83ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac988b5-35c7-462d-a1ef-6c8a73278cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model, task, and prompt\n",
    "pretrains = ['google/flan-t5-large']\n",
    "datasets = ['cola']\n",
    "prompt_types = ['standard_a']\n",
    "\n",
    "for pretrained in pretrains:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained, padding_side='left')\n",
    "    tokenizer.add_special_tokens({'pad_token': pad_tokens[pretrained]})        \n",
    "    if pretrained in ['google/flan-t5-large', 'google/flan-t5-xxl']:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(pretrained).to(device)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained).to(device)\n",
    "    for dataset in datasets:\n",
    "        for prompt_type in prompt_types:\n",
    "            for n_o_s in number_of_shots[dataset]:\n",
    "                for seed in seeds:\n",
    "                    \n",
    "                    config = {\n",
    "                        'experiment_id': time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n",
    "                        'dataset': dataset,\n",
    "                        'number_of_data': None,\n",
    "                        'model': pretrained,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'number_of_shots': n_o_s,\n",
    "                        'temperature': 0.8,\n",
    "                        'max_tokens': max_tokens[prompt_type],\n",
    "                        'batch_size': 16,\n",
    "                        'pad_token_id': pad_ids[pretrained],\n",
    "                        'eos_token_id': eos_ids[pretrained],\n",
    "                        'seed': seed,\n",
    "                        'device': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "                    }\n",
    "\n",
    "                    random.seed(seed)\n",
    "                    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "                    # read in the prompt\n",
    "                    with open(f'prompts/{dataset}/{prompt_type}-{n_o_s}.txt') as file:\n",
    "                        prefix = file.read()\n",
    "                        \n",
    "                    # read in the data\n",
    "                    with open(f'datasets/{dataset}.json') as file:\n",
    "                        data = json.loads(file.read())\n",
    "                    config['number_of_data'] = len(data)\n",
    "\n",
    "                    # inference\n",
    "                    results = defaultdict(dict)\n",
    "                    with tqdm(total=config['number_of_data']) as t:\n",
    "                        for i, item in data.items():\n",
    "                            inputs = []\n",
    "                            inputs.append(concatenate(dataset, prompt_type, prefix, item['original'], item))\n",
    "                            for synthetic in list(item['synthetic'].values())[:4]:\n",
    "                                inputs.append(concatenate(dataset, prompt_type, prefix, synthetic, item))\n",
    "                            input_ids = tokenizer(inputs, padding=True, return_tensors='pt').input_ids.to(device)\n",
    "                            loader = DataLoader(input_ids, batch_size=config['batch_size'], shuffle=False)\n",
    "                            outputs = []\n",
    "                            for input in loader:\n",
    "                                with torch.no_grad():\n",
    "                                    output = model.generate(input, max_new_tokens=config['max_tokens'], temperature=config['temperature'], pad_token_id=config['pad_token_id'], do_sample=True, top_k=20)\n",
    "                                outputs.extend(tokenizer.batch_decode(output))\n",
    "                            results[i]['original'] = item['original']\n",
    "                            results[i]['label'] = item['label']\n",
    "                            results[i]['input'] = inputs[0]\n",
    "                            results[i]['prediction_original'] = outputs[:1]\n",
    "                            results[i]['prediction_synthetic'] = outputs[1:]\n",
    "                            results[i]['synthetic'] = list(item['synthetic'].values())[:4]\n",
    "                            results[i]['input_synthetic'] = inputs[1:5]\n",
    "                            t.update(1)\n",
    "\n",
    "                    # save the results\n",
    "                    with open('config_reference.txt', 'a') as file:\n",
    "                        file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "                        file.close()\n",
    "                    with open('results/results-{}.json'.format(config['experiment_id']), 'w') as file:\n",
    "                        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f5f30-2cd8-4767-b02a-31c559be1428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
