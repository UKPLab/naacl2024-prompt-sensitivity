{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f467238-789c-41fd-9c1c-d15b80d83ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac988b5-35c7-462d-a1ef-6c8a73278cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model, task, and prompt\n",
    "pretrains = ['google/flan-t5-large']\n",
    "datasets = ['cola']\n",
    "prompt_types = ['standard_a']\n",
    "seeds = [2266]\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for pretrained in pretrains:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained, padding_side='left')\n",
    "    tokenizer.add_special_tokens({'pad_token': pad_tokens[pretrained]})\n",
    "    if pretrained in ['google/flan-t5-large', 'google/flan-t5-xxl']:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(pretrained).to(device)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained).to(device)\n",
    "    for dataset in datasets:\n",
    "        for prompt_type in prompt_types:\n",
    "            for n_o_s in number_of_shots[dataset]:\n",
    "                for seed in seeds:\n",
    "                    for a in alphas:\n",
    "                        \n",
    "                        config = {\n",
    "                            'experiment_id': time.strftime('%Y%m%d_%H%M%S', time.localtime()),\n",
    "                            'dataset': dataset,\n",
    "                            'number_of_data': None,\n",
    "                            'model': pretrained,\n",
    "                            'prompt_type': prompt_type,\n",
    "                            'number_of_shots': n_o_s,\n",
    "                            'a': a,\n",
    "                            'max_tokens': max_tokens[prompt_type],\n",
    "                            'batch_size': 16,\n",
    "                            'pad_token_id': pad_ids[pretrained],\n",
    "                            'eos_token_id': eos_ids[pretrained],\n",
    "                            'seed': seed,\n",
    "                            'device': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "                            'note': 'sad'\n",
    "                        }\n",
    "\n",
    "                        random.seed(seed)\n",
    "                        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "                        # read in the prompt\n",
    "                        with open(f'prompts/{dataset}/{prompt_type}-{n_o_s}.txt') as file:\n",
    "                            prefix = file.read()\n",
    "                            \n",
    "                        # read in the data\n",
    "                        with open(f'datasets/{dataset}.json') as file:\n",
    "                            data = json.loads(file.read())\n",
    "                        config['number_of_data'] = len(data)\n",
    "\n",
    "                        # inference\n",
    "                        results = defaultdict(dict)\n",
    "                        with tqdm(total=config['number_of_data']) as t:\n",
    "                            for i, item in data.items():\n",
    "                                inputs = []\n",
    "                                inputs.append(concatenate(dataset, prompt_type, prefix, item['original'], item))\n",
    "                                for synthetic in list(item['synthetic'].values())[:4]:\n",
    "                                    inputs.append(concatenate(dataset, prompt_type, prefix, synthetic, item))\n",
    "                                input_ids = tokenizer(inputs, padding=True, return_tensors='pt').input_ids.to(device)\n",
    "                                loader = DataLoader(input_ids, batch_size=config['batch_size'], shuffle=False)\n",
    "                                outputs = []\n",
    "                                for input in loader:\n",
    "                                    with torch.no_grad():\n",
    "                                        output = model.generate(\n",
    "                                            input,\n",
    "                                            max_new_tokens=config['max_tokens'],\n",
    "                                            pad_token_id=config['pad_token_id'],\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            output_scores=True\n",
    "                                        )\n",
    "                                    if pretrained in ['google/flan-t5-large', 'google/flan-t5-xxl']:\n",
    "                                        outputs.extend(tokenizer.batch_decode(output.sequences))\n",
    "                                    else:\n",
    "                                        outputs.extend(tokenizer.batch_decode(output.sequences[:, input_ids.shape[1]:]))\n",
    "                                results[i]['original'] = item['original']\n",
    "                                results[i]['label'] = item['label']\n",
    "                                results[i]['input'] = inputs[0]\n",
    "                                results[i]['prediction_original'] = outputs[:1]\n",
    "                                results[i]['prediction_synthetic'] = outputs[1:]\n",
    "                                results[i]['prediction_sad'] = tokenizer.decode(torch.argmax(config['a'] * output.scores[0][0] - (1-config['a']) * torch.var(output.scores[0][1:], dim=0))) ## the prediction obtained using sensitivity-aware decoding\n",
    "                                if results[i]['prediction_sad'] in ['', ' ', '\\n']:\n",
    "                                    results[i]['prediction_sad'] = tokenizer.decode(torch.argmax(config['a'] * output.scores[1][0] - (1-config['a']) * torch.var(output.scores[1][1:], dim=0)))\n",
    "                                results[i]['synthetic'] = list(item['synthetic'].values())[:4]\n",
    "                                results[i]['input_synthetic'] = inputs[1:5]\n",
    "                                t.update(1)\n",
    "\n",
    "                        # save the results\n",
    "                        with open(f'config_reference_sad.txt', 'a') as file:\n",
    "                            file.write(('\\t'.join(['{' + i + '}' for i in config.keys()]) + '\\n').format(**config))\n",
    "                            file.close()\n",
    "                        with open('results_sad/results-{}.json'.format(config['experiment_id']), 'w') as file:\n",
    "                            json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57d926-4657-41c3-ada5-0015c1be1f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
